{
 "metadata": {
  "name": "",
  "signature": "sha256:a210406edc1ee90b6e9be22e984289725e2438a0f2d562f3ea1cb214bd1012bb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classes and functions for the SardiNIA Age Rates Project"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import pylab as pl\n",
      "import pickle\n",
      "import os\n",
      "import sys\n",
      "import scipy\n",
      "from scipy.stats import pearsonr\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from collections import Counter\n",
      "from sklearn.lda import LDA"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class WaveInfo (object):\n",
      "    class WaveData (object):\n",
      "        def __init__ (self):\n",
      "            raw_data         = None\n",
      "            data_matrix      = None\n",
      "            col_names        = None\n",
      "            age_col_idx      = None\n",
      "            id_col_idx       = None\n",
      "            data_class_views = None\n",
      "            class_vals       = None\n",
      "            n_features       = None\n",
      "        # A summary of the WaveData object\n",
      "        def __str__ (self):\n",
      "            s = \"{} samples, {} features, {} classes\\n\".format(\n",
      "                self.data_matrix.shape[0],self.data_matrix.shape[1],len(self.data_class_views))\n",
      "            vals_str = \"\"\n",
      "            counts_str = \"\"\n",
      "            for i in range(len(self.data_class_views)):\n",
      "                vals_str   += \"{:6.1f}\".format(self.class_vals[i])\n",
      "                counts_str += \"{:4d}  \".format(len(self.data_class_views[i]))\n",
      "            s += vals_str   + \"\\n\"\n",
      "            s += counts_str + \"\\n\"\n",
      "            return (s)\n",
      "\n",
      "    def __init__ (self, **kwargs):\n",
      "        self.wavenum        = kwargs.get('wavenum'       , 0)\n",
      "        self.filename_base  = kwargs.get('filename_base' , \"\")\n",
      "        self.raw_data_fname = kwargs.get('raw_data_fname', \"\")\n",
      "        self.num_train      = kwargs.get('num_train'     , 0)\n",
      "        self.num_test       = kwargs.get('num_test'      , 0)\n",
      "        self.age_start      = kwargs.get('age_start'     , 0)\n",
      "        self.age_end        = kwargs.get('age_end'       , 100)\n",
      "        self.bin_years      = kwargs.get('bin_years'     , 5)\n",
      "        self.read_func      = kwargs.get('read_func'     , None)\n",
      "        # This is a dict of ModelName : Best # of features\n",
      "        self.best_models    = kwargs.get('best_models'   , {})\n",
      "        # A cached read of the raw and cleaned-up data\n",
      "        self.wave_data      = kwargs.get('wave_data'     , None)\n",
      "\n",
      "    def raw_data_path (self):\n",
      "        if len (self.raw_data_fname) > 1:\n",
      "            return (os.path.join (Config.data_dir(), self.raw_data_fname))\n",
      "        else:\n",
      "            return (os.path.join (Config.data_dir(),\n",
      "                Config.data_dist_base+'-Wave{}.{}'.format(\n",
      "                self.wavenum,Config.wave_data_file_ext))\n",
      "            )\n",
      "\n",
      "    # These are defined later, but set right here\n",
      "    # fake way of forward-declaration in python.\n",
      "    def read_data_v1 (wavenum, what):\n",
      "        print ('reading data from ',Config.wave_info(wavenum).raw_data_path())\n",
      "        sys.stdout.flush()\n",
      "        return (read_data_updated (Config.wave_info(wavenum).raw_data_path(), what))\n",
      "\n",
      "    def read_data_v2 (wavenum):\n",
      "        print ('reading data from ',Config.wave_info(wavenum).raw_data_path())\n",
      "        sys.stdout.flush()\n",
      "        return (read_w2 (Config.wave_info(wavenum).raw_data_path()))\n",
      "\n",
      "    def read_data_v3 (wavenum, what):\n",
      "        print ('reading data from ',Config.wave_info(wavenum).raw_data_path())\n",
      "        sys.stdout.flush()\n",
      "        return (read_data_updated2 (Config.wave_info(wavenum).raw_data_path(), what))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Configuration\n",
      "This includes various values/settings/etc determined empirically as the project progressed"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Config (object):\n",
      "    data_dir_path = 'data'\n",
      "    result_dir_path = 'results' # relative to data_dir_path\n",
      "    graph_dir_path = 'Graphs' # relative to data_dir_path\n",
      "# Directory for data files is returned by data_dir ():\n",
      "#     return (os.path.join (Config.data_dir_path,Config.data_dist_base))\n",
      "# The default raw data filename is set to:\n",
      "#    data_dist_base+'-Wave{wavenum}.{wave_data_file_ext}\n",
      "#    e.g. \"2014-08-26-Sardinia-Wave1.tsv\"\n",
      "# The default raw data filename can be set using the raw_data_fname field, e.g.:\n",
      "#    raw_data_fname = \"newWave1.csv\"\n",
      "\n",
      "    data_dist_base = '2014-08-26-Sardinia'\n",
      "#    data_dist_base = '2013-12-18-Sardinia-CleanUp-Data'\n",
      "    mrmr_bin = os.path.join (os.path.expanduser(\"~\"),'bin','mrmr')\n",
      "    mrmr_sfx = '_mrmr_RS'   # change to +\"_mrmr_10\" for old splits, \"_mrmr\" for non-random-seed splits\n",
      "    split_num_fmt = '_s{0:04d}'\n",
      "    \n",
      "    wave_data_file_ext = 'tsv'\n",
      "    wave_data_file_delim = '\\t'\n",
      "\n",
      "    # N.B.: by default, wave_infos[wavenum].raw_data_fname = \"\"\n",
      "    dist_wave_infos = {\n",
      "        '2013-12-18-Sardinia-CleanUp-Data' : {\n",
      "            1 : WaveInfo (\n",
      "                wavenum        = 1,\n",
      "                filename_base  = \"sard_w1_split_120tr_13te\",\n",
      "                num_train      = 120,\n",
      "                num_test       =  13,\n",
      "                age_start      =  12,\n",
      "                age_end        =  77,\n",
      "                bin_years      =   5,\n",
      "                read_func      = lambda: WaveInfo.read_data_v1 (1, 'features'),\n",
      "                best_models    = {\n",
      "                    'RandForClf':120,\n",
      "                    'KNeighReg' :121,\n",
      "                    'NuSVC'     :90, # not determined - made up!\n",
      "                    'SVC'       :75, # not determined - made up!\n",
      "                },\n",
      "            ),\n",
      "            2 : WaveInfo (\n",
      "                wavenum        =  2,\n",
      "                filename_base  = \"sard_w2_split_96tr_10te\",\n",
      "                num_train      = 96,\n",
      "                num_test       = 10,\n",
      "                age_start      = 16,\n",
      "                age_end        = 81,\n",
      "                bin_years      =  5,\n",
      "                read_func      = lambda: WaveInfo.read_data_v2 (2),\n",
      "                best_models    = {\n",
      "                    'RandForClf' : 95,\n",
      "                    'KNeighReg'  : 91,\n",
      "                    'NuSVC'      : 90, # not determined - made up!\n",
      "                    'SVC'        : 75, # not determined - made up!\n",
      "                },\n",
      "            ),\n",
      "            3 : WaveInfo (\n",
      "                wavenum        =  3,\n",
      "                filename_base  = \"sard_w3_split_82tr_8te\",\n",
      "                num_train      = 82,\n",
      "                num_test       =  8,\n",
      "                age_start      = 20,\n",
      "                age_end        = 80,\n",
      "                bin_years      =  5,\n",
      "                read_func      = lambda: WaveInfo.read_data_v1 (3, 'samples'),\n",
      "                best_models    = {\n",
      "                    'RandForClf': 84,\n",
      "                    'KNeighReg' : 85,\n",
      "                },\n",
      "            ),\n",
      "        },\n",
      "        '2014-08-26-Sardinia' : {\n",
      "            1 : WaveInfo (\n",
      "                wavenum        = 1,\n",
      "                filename_base  = \"sard_w1_split_119tr_13te\",\n",
      "                num_train      = 119,\n",
      "                num_test       =  13,\n",
      "                age_start      =  15,\n",
      "                age_end        =  80,\n",
      "                bin_years      =   5,\n",
      "                read_func      = lambda: WaveInfo.read_data_v3 (1, 'samples'),\n",
      "                best_models    = {\n",
      "                    'RandForClf':120,\n",
      "                    'KNeighReg' :121,\n",
      "                    'NuSVC'     :90, # not determined - made up!\n",
      "                    'SVC'       :75, # not determined - made up!\n",
      "                },\n",
      "            ),\n",
      "            2 : WaveInfo (\n",
      "                wavenum        =  2,\n",
      "                filename_base  = \"sard_w2_split_161tr_18te\",\n",
      "                num_train      = 161,\n",
      "                num_test       = 18,\n",
      "                age_start      = 15,\n",
      "                age_end        = 80,\n",
      "                bin_years      =  5,\n",
      "                read_func      = lambda: WaveInfo.read_data_v3 (2, 'samples'),\n",
      "                best_models    = {\n",
      "                    'RandForClf' : 95,\n",
      "                    'KNeighReg'  : 91,\n",
      "                    'NuSVC'      : 90, # not determined - made up!\n",
      "                    'SVC'        : 75, # not determined - made up!\n",
      "                },\n",
      "            ),\n",
      "            3 : WaveInfo (\n",
      "                wavenum        =   3,\n",
      "                filename_base  = \"sard_w3_split_103tr_12te\",\n",
      "                num_train      = 103,\n",
      "                num_test       =  12,\n",
      "                age_start      =  18,\n",
      "                age_end        =  83,\n",
      "                bin_years      =   5,\n",
      "                read_func      = lambda: WaveInfo.read_data_v3 (3, 'samples'),\n",
      "                best_models    = {\n",
      "                    'RandForClf': 95, # made up, based on wave 2!\n",
      "                    'KNeighReg' : 91, # made up, based on wave 2!\n",
      "                },\n",
      "            ),\n",
      "        },\n",
      "    }\n",
      "\n",
      "    wave_infos = dict (dist_wave_infos [data_dist_base])\n",
      "\n",
      "    def set_data_dist_base (data_dist_base):\n",
      "        if data_dist_base in ['2014-08-26-Sardinia','2014-08-26']:\n",
      "            Config.data_dist_base = '2014-08-26-Sardinia'\n",
      "        elif data_dist_base in ['2013-12-18-Sardinia-CleanUp-Data','2013-12-18-Sardinia','2013-12-18']:\n",
      "            Config.data_dist_base = '2013-12-18-Sardinia-CleanUp-Data'\n",
      "        print ('Setting Config.data_dist_base to \"{}\"'.format (Config.data_dist_base))\n",
      "        Config.wave_infos = dict (Config.dist_wave_infos [Config.data_dist_base])\n",
      "\n",
      "    def data_dir ():\n",
      "       return (os.path.join (Config.data_dir_path,Config.data_dist_base))\n",
      "    \n",
      "    def result_dir ():\n",
      "       return (os.path.join (Config.data_dir(), Config.result_dir_path))\n",
      "    \n",
      "    def graph_dir ():\n",
      "       return (os.path.join (Config.data_dir(), Config.graph_dir_path))\n",
      "\n",
      "    \n",
      "    def wave_info (wavenum):\n",
      "        Split.filename_base = Config.wave_infos[wavenum].filename_base\n",
      "        return (Config.wave_infos[wavenum])\n",
      "\n",
      "    def wave_data (wavenum):\n",
      "        wi = Config.wave_info (wavenum)\n",
      "        if wi.wave_data is None:\n",
      "            wd = WaveInfo.WaveData()\n",
      "            wd.raw_data = wi.read_func()\n",
      "            ages = wd.raw_data.Age.values\n",
      "            wd.data_matrix, wd.col_names, wd.age_col_idx, wd.id_col_idx = clean_convert(wd.raw_data)\n",
      "            wd.data_class_views, wd.class_vals = bin_data (wi.age_start, wi.age_end, wi.bin_years, wd.data_matrix, ages)\n",
      "            wd.n_features = len (wd.col_names) - 2\n",
      "            # cache the result\n",
      "            wi.wave_data = wd\n",
      "        return (wi.wave_data)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Main Class for holding Split information"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Split (object):\n",
      "    filename_base = Config.wave_infos[1].filename_base\n",
      "    filename_base_pro2 = filename_base+\"_pro2\"\n",
      "    def __init__ (self):\n",
      "        # These are 2D numpy arrays, features in columns, samples in rows\n",
      "        self.train_set = None\n",
      "        self.test_set = None\n",
      "        self.n_classes = None\n",
      "\n",
      "        # classed_labels are center values for each bin\n",
      "        # labels are the continuously varying ages (unbinned) \n",
      "        self.test_labels = []\n",
      "        self.train_labels = []\n",
      "        self.test_classed_labels = []\n",
      "        self.train_classed_labels = []\n",
      "        # cached dict of class values\n",
      "        self.class_vals = None\n",
      "        # Participant IDs\n",
      "        self.train_id = []\n",
      "        self.test_id = []\n",
      "        \n",
      "        # N.B.: Anything added here must be added to copy()!\n",
      "\n",
      "#    def copy (self,from_sp):\n",
      "#        self.train = from_sp.train\n",
      "#        self.test = from_sp.test\n",
      "#        self.test_labels = from_sp.test_labels\n",
      "#        self.test_classed_labels = from_sp.test_classed_labels\n",
      "#        self.train_labels = from_sp.train_labels\n",
      "#        self.train_classed_labels = from_sp.train_classed_labels\n",
      "\n",
      "#        self.train_vstack = from_sp.train_vstack\n",
      "#        self.test_vstack = from_sp.test_vstack\n",
      "#        self.train_3d = from_sp.train_3d\n",
      "#        self.test_3d = from_sp.test_3d\n",
      "\n",
      "#        self.sorted_train = from_sp.sorted_train\n",
      "#        self.sorted_test = from_sp.sorted_test\n",
      "#        self.stand_train = from_sp.stand_train\n",
      "#        self.stand_test = from_sp.stand_test\n",
      "#        self.weigh_train = from_sp.weigh_train\n",
      "#        self.weigh_test = from_sp.weigh_test\n",
      "#        return (self)\n",
      "    \n",
      "    def copy (self,from_sp):\n",
      "        # This will still not make actual copies of numpys burried in lists!!!\n",
      "        # They will be references to the same original numpys!\n",
      "        for k in from_sp.__dict__.keys():\n",
      "            if (type(from_sp.__dict__[k]) == np.ndarray):\n",
      "                self.__dict__[k] = from_sp.__dict__[k].copy()\n",
      "            else:\n",
      "                self.__dict__[k] = from_sp.__dict__[k]\n",
      "        return (self)\n",
      "\n",
      "    def get_fname (self, splitnum, protocol = 3):\n",
      "        if protocol == 3:\n",
      "            fname = Split.filename_base+Config.split_num_fmt.format(splitnum)+\".pickle\"\n",
      "            return (os.path.join(Config.data_dir(),Split.filename_base,fname))\n",
      "        else:\n",
      "            fname_pro2 = Split.filename_base_pro2+Config.split_num_fmt.format(splitnum)+\".pickle\"\n",
      "            return (os.path.join(Config.data_dir(),Split.filename_base_pro2,fname_pro2))\n",
      "        \n",
      "    def save (self, filename, protocol=3):\n",
      "        with open(filename, 'wb') as f:\n",
      "            pickle.dump(self, f, protocol)\n",
      "\n",
      "    def read (self, filename):\n",
      "        if sys.version_info >= (3, 0):\n",
      "            split = self.read_py3_pickle(filename)\n",
      "        else:\n",
      "            split = self.read_py2_from_py3_pickle(filename)\n",
      "\n",
      "        # This is to read older versions of Split\n",
      "        if split.test_set is None and not split.test is None:\n",
      "            split.test_set = np.vstack(split.test)\n",
      "        if split.train_set is None and not split.train is None:\n",
      "            split.train_set = np.vstack(split.train)\n",
      "        return (self)\n",
      "\n",
      "    def read_py3_pickle (self, filename):\n",
      "        rd = None\n",
      "        with open(filename, 'rb') as f:\n",
      "            rd = pickle.load(f)\n",
      "        return (self.copy (rd))\n",
      "    \n",
      "    def read_py2_from_py3_pickle (self, filename):\n",
      "        rd = None\n",
      "        with open(filename, 'rb') as f:\n",
      "            rd = load_py2_from_py3_pickle(f)\n",
      "        return (self.copy (rd))\n",
      "\n",
      "    def train_test(self, train_amount, test_amount, views, class_vals, age_col_idx, id_col_idx, rand_seed = None):\n",
      "        train = []\n",
      "        test = []\n",
      "        train_labels = []\n",
      "        train_classed_labels = []\n",
      "        test_labels = []\n",
      "        test_classed_labels = []\n",
      "        train_id = []\n",
      "        test_id = []\n",
      "        index = 0\n",
      "        for view in views:\n",
      "            bv = class_vals[index]\n",
      "            index = index + 1 \n",
      "\n",
      "            new_view = np.insert(view, 0, bv, axis=1)\n",
      "            # This insert makes age_col_idx and id_col_idx increase by 1.\n",
      "            new_age_col_idx = age_col_idx+1\n",
      "            new_id_col_idx = id_col_idx+1\n",
      "            if rand_seed is not None:\n",
      "                random = np.random.RandomState(rand_seed).permutation(new_view)\n",
      "            else:\n",
      "                random = np.random.permutation(new_view)\n",
      "            train_classed_labels.append(random[:train_amount,0])\n",
      "            test_classed_labels.append(random[train_amount:train_amount+test_amount:,0])\n",
      "            train_labels.append(random[:train_amount, new_age_col_idx])\n",
      "            test_labels.append(random[train_amount:train_amount+test_amount:, new_age_col_idx])\n",
      "            train_id.append(random[:train_amount, new_id_col_idx])\n",
      "            test_id.append(random[train_amount:train_amount+test_amount:,new_id_col_idx])\n",
      "\n",
      "            class_data = np.delete(random, new_age_col_idx , axis=1)\n",
      "            class_data = np.delete(class_data, new_id_col_idx , axis=1)\n",
      "            class_data = np.delete(class_data, 0 , axis=1)\n",
      "\n",
      "            train.append(class_data[:train_amount, :])\n",
      "            test.append(class_data[train_amount:train_amount+test_amount, :]) \n",
      "        \n",
      "        self.train_labels = np.concatenate(train_labels)\n",
      "        self.train_classed_labels = np.concatenate(train_classed_labels)\n",
      "        self.test_labels = np.concatenate(test_labels)\n",
      "        self.test_classed_labels = np.concatenate(test_classed_labels)\n",
      "        self.train_id = np.concatenate(train_id)\n",
      "        self.test_id = np.concatenate(test_id)\n",
      "\n",
      "        self.train_set = np.vstack(train)\n",
      "        self.test_set = np.vstack(test)\n",
      "        return (self)\n",
      "\n",
      "    def load_wave_train_test_RS (self, wavenum, split_num):\n",
      "        wi = Config.wave_info (wavenum)\n",
      "        wd = Config.wave_data (wavenum)\n",
      "        self.train_test(wi.num_train, wi.num_test,\n",
      "            wd.data_class_views, wd.class_vals, wd.age_col_idx, wd.id_col_idx,\n",
      "            split_num)\n",
      "\n",
      "    def get_class_vals(self):\n",
      "        if self.class_vals is None:\n",
      "            class_vals_dict = {}\n",
      "            for class_label in self.train_classed_labels:\n",
      "                class_vals_dict[class_label] = None\n",
      "            self.class_vals = sorted(class_vals_dict.keys())\n",
      "            self.n_classes = len (self.class_vals)\n",
      "        return (self.class_vals)\n",
      "\n",
      "    def get_n_classes(self):\n",
      "        if self.n_classes is None:\n",
      "            self.get_class_vals()\n",
      "        return (self.n_classes)\n",
      "\n",
      "    def get_train_3d (self):\n",
      "        if self.train_3d is None:\n",
      "            self.train_3d = get_class_mat_list (self.train_set, self.train_classed_labels)\n",
      "        return (self.train_3d)\n",
      "\n",
      "    def get_test_3d (self):\n",
      "        if self.test_3d is None:\n",
      "            self.test_3d = get_class_mat_list (self.test_set, self.test_classed_labels)\n",
      "        return (self.test_3d)\n",
      "\n",
      "    def sort_stand_weigh(self, feature_weights):\n",
      "        i = np.argsort(feature_weights)\n",
      "        sorted_train = self.get_train_vstack()[:,i]\n",
      "        self.sorted_train = np.fliplr(sorted_train)\n",
      "        sorted_test = self.get_test_vstack()[:,i]\n",
      "        self.sorted_test = np.fliplr(sorted_test)\n",
      "        self.stand(self.sorted_train, self.sorted_test)\n",
      "        self.weigh_train = np.multiply(self.stand_train, feature_weights)\n",
      "        self.weigh_test = np.multiply(self.stand_test, feature_weights)\n",
      "        return (self)\n",
      "\n",
      "    def norm_weigh_sort(self, feature_weights):\n",
      "        self.normalize (self.train_set, self.test_set)\n",
      "        self.apply_weights (feature_weights)\n",
      "        sorted_weights = self.sort_by_weight (feature_weights)\n",
      "        return (sorted_weights)\n",
      "\n",
      "    def stand_sort(self, feature_weights):\n",
      "        self.stand (self.train_set, self.test_set)\n",
      "        sorted_weights = self.sort_by_weight (feature_weights)\n",
      "        return (sorted_weights)\n",
      "\n",
      "    def stand_weigh_sort(self, feature_weights):\n",
      "        self.stand (self.train_set, self.test_set)\n",
      "        self.apply_weights (feature_weights)\n",
      "        sorted_weights = self.sort_by_weight (feature_weights)\n",
      "        return (sorted_weights)\n",
      "\n",
      "    def get_trimmed_features (self, num_feat):\n",
      "        new_train = self.train_set[:,:num_feat]\n",
      "        new_test = self.test_set[:,:num_feat]\n",
      "        return (new_train, new_test)\n",
      "\n",
      "    def stand (self, train, test):\n",
      "        scaler = StandardScaler()\n",
      "        self.train_set = scaler.fit_transform(train)\n",
      "        self.test_set = scaler.transform(test)\n",
      "        return (self)\n",
      "    \n",
      "    def normalize (self, train, test):\n",
      "        self.train_set = train.copy() \n",
      "        mins, maxs = normalize_by_columns (self.train_set)\n",
      "        self.test_set = test.copy() \n",
      "        normalize_by_columns (self.test_set, mins, maxs)\n",
      "        return (self)\n",
      "\n",
      "    def sort_by_weight (self, feature_weights):\n",
      "        i = np.argsort(feature_weights)\n",
      "        self.train_set = self.train_set[:,i]\n",
      "        self.train_set = np.fliplr(self.train_set)\n",
      "        self.test_set = self.test_set[:,i]\n",
      "        self.test_set = np.fliplr(self.test_set)\n",
      "        feature_weights = np.sort(feature_weights)\n",
      "        feature_weights[:] = feature_weights[::-1]\n",
      "        return (feature_weights)\n",
      "\n",
      "    def apply_weights (self, feature_weights):\n",
      "        self.train_set = np.multiply (self.train_set, feature_weights)\n",
      "        self.test_set = np.multiply (self.test_set, feature_weights)\n",
      "        return (self)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_class_mat_list (mat, class_labels):\n",
      "    assert (len(mat) == len(class_labels))\n",
      "    class_label_dict = {}\n",
      "    class_mats = []\n",
      "    class_label_idx = 0\n",
      "    for samp_idx in range (len (mat)):\n",
      "        class_label = class_labels[samp_idx]\n",
      "        if not class_label in class_label_dict:\n",
      "            class_label_dict[class_label] = class_label_idx\n",
      "            class_mats.append (mat[samp_idx])\n",
      "            class_label_idx += 1\n",
      "        else:\n",
      "            class_idx = class_label_dict[class_label]\n",
      "            class_mats[class_idx] = np.vstack ([class_mats[class_idx],mat[samp_idx]])\n",
      "    return (np.array(class_mats))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_data(file_name):\n",
      "    x = pd.read_csv(file_name,sep=Config.wave_data_file_delim, na_values=[])\n",
      "    x = x.dropna(axis=1, how='all')\n",
      "    x = x.dropna(how='all')\n",
      "    y, z = x.shape\n",
      "    x = x.dropna(axis=1, thresh=(y/3))\n",
      "    y, z = x.shape\n",
      "    x = x.dropna(thresh=(z/2))\n",
      "    y, z = x.shape\n",
      "    x = x.dropna(axis=1, thresh=(y*2/3))\n",
      "    y, z = x.shape\n",
      "    x = x.dropna(thresh=(z*3/4))\n",
      "    y, z = x.shape\n",
      "    x = x.dropna(axis=1, thresh=(y*9/10))\n",
      "    y, z = x.shape\n",
      "    x = x.dropna(thresh=(z*4/5))\n",
      "    y, z = x.shape\n",
      "    x = x.dropna(axis=1, thresh=(y*9/10))\n",
      "    y, z = x.shape\n",
      "    x = x.dropna(how='any')\n",
      "    return x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_data_updated(file_name, what_to_prioritize):\n",
      "    x = pd.read_csv(file_name,sep=Config.wave_data_file_delim, na_values=[])\n",
      "    x = x.dropna(axis=1, how='all')\n",
      "    x = x.dropna(how='all')\n",
      "    x = x[pd.notnull(x['pwv'])]\n",
      "    x = x[pd.notnull(x['exmBMI'])]\n",
      "    if 'exmBPsys_jbs' in x:\n",
      "        x = x[pd.notnull(x['exmBPsys_jbs'])]\n",
      "    else:\n",
      "        x = x[pd.notnull(x['exmBPsys'])]\n",
      "    x = x[pd.notnull(x['exmWaist'])]\n",
      "    x = x[pd.notnull(x['labsColesterolo'])]\n",
      "    threshold_num = 0.05\n",
      "    if what_to_prioritize is 'features':\n",
      "        for i in range (47):\n",
      "            x = x.dropna(thresh=(x.shape[1]*threshold_num))\n",
      "            x = x.dropna(axis=1, thresh=(x.shape[0]*threshold_num))\n",
      "            threshold_num += 0.02\n",
      "        x = x.dropna(how='any')\n",
      "    elif what_to_prioritize is 'samples':\n",
      "        for i in range (47):\n",
      "            x = x.dropna(axis=1, thresh=(x.shape[0]*threshold_num))\n",
      "            x = x.dropna(thresh=(x.shape[1]*threshold_num))\n",
      "            threshold_num += 0.02\n",
      "        x = x.dropna(axis=1, how='any')\n",
      "    else:\n",
      "        return(\"Need to specify what to prioritize: 'features' or 'samples'\")\n",
      "    return (x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_data_updated2(file_name, what_to_prioritize):\n",
      "    x = pd.read_csv(file_name,sep=Config.wave_data_file_delim, na_values=[])\n",
      "    x = x.dropna(axis=1, how='all')\n",
      "    x = x.dropna(how='all')\n",
      "    # We're not singling out \"must keep\" columns like before:\n",
      "    #   pwv, exmBMI, exmBPsys_jbs, exmBPsys, exmWaist, labsColesterolo\n",
      "    # Insead we're dropping unwanted columns first.\n",
      "    x = clean_data (x)\n",
      "\n",
      "    threshold_num = 0.05\n",
      "    if what_to_prioritize is 'features':\n",
      "        for i in range (47):\n",
      "            x = x.dropna(thresh=(x.shape[1]*threshold_num))\n",
      "            x = x.dropna(axis=1, thresh=(x.shape[0]*threshold_num))\n",
      "            threshold_num += 0.02\n",
      "        x = x.dropna(how='any')\n",
      "    elif what_to_prioritize is 'samples':\n",
      "        for i in range (47):\n",
      "            x = x.dropna(axis=1, thresh=(x.shape[0]*threshold_num))\n",
      "            x = x.dropna(thresh=(x.shape[1]*threshold_num))\n",
      "            threshold_num += 0.02\n",
      "        x = x.dropna(axis=1, how='any')\n",
      "    else:\n",
      "        return(\"Need to specify what to prioritize: 'features' or 'samples'\")\n",
      "    return (x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_w2 (file_name):\n",
      "    x = pd.read_csv(file_name,sep=Config.wave_data_file_delim, na_values=[])\n",
      "    x = x.dropna(axis=1, how='all')\n",
      "    x = x.dropna(how='all')\n",
      "    x = x[pd.notnull(x['exmBMI'])]\n",
      "    if 'exmBPsys_jbs' in x:\n",
      "        x = x[pd.notnull(x['exmBPsys_jbs'])]\n",
      "    else:\n",
      "        x = x[pd.notnull(x['exmBPsys'])]\n",
      "    x = x[pd.notnull(x['exmWaist'])]\n",
      "    x = x[pd.notnull(x['labsColesterolo'])]\n",
      "    x = x[pd.notnull(x['vasPSV'])]\n",
      "    x = x[pd.notnull(x['vasIMT'])]\n",
      "    x = x[pd.notnull(x['vasvti'])]\n",
      "    y, z = x.shape\n",
      "    x = x.dropna(axis=1, thresh=(y/3))\n",
      "    y, z = x.shape\n",
      "    x = x.dropna(thresh=(z/2))\n",
      "    y, z = x.shape\n",
      "    x = x.dropna(axis=1, thresh=(y/2))\n",
      "    y, z = x.shape\n",
      "    x = x.dropna(thresh=(z*4/5))\n",
      "    y, z = x.shape\n",
      "    x = x.dropna(axis=1, thresh=(y*9/10))\n",
      "    y, z = x.shape\n",
      "    x = x.dropna(thresh=(z*9/10))\n",
      "    y, z = x.shape\n",
      "    x = x.dropna(axis=1, thresh=(y*99/100))\n",
      "    y, z = x.shape\n",
      "    x = x.dropna(how='any')\n",
      "    return (x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Unfinished harmonized feature function\n",
      "#def read_data_updated_harmonized(file_name, what_to_prioritize):\n",
      "#    x = pd.read_csv(file_name, na_values=[])\n",
      "#    x = x.dropna(axis=1, how='all')\n",
      "#    x = x.dropna(how='all')\n",
      "#    x = x[pd.notnull(x['pwv'])]\n",
      "#    x = x[pd.notnull(x['exmBMI'])]\n",
      "#    x = x[pd.notnull(x['exmBPsys_jbs'])]\n",
      "#    x = x[pd.notnull(x['exmWaist'])]\n",
      " #   x = x[pd.notnull(x['labsColesterolo'])]\n",
      "  #  x = x[pd.notnull(x['vasPSV'])]\n",
      "   # x = x[pd.notnull(x['vasIMT'])]\n",
      "#    x = x[pd.notnull(x['vasvti'])]\n",
      " #   threshold_num = 0.05\n",
      "  #  if what_to_prioritize is 'features':\n",
      "   #     for i in range (47):\n",
      "#            x = x.dropna(thresh=(x.shape[1]*threshold_num))\n",
      " #           x = x.dropna(axis=1, thresh=(x.shape[0]*threshold_num))\n",
      "#         threshold_num += 0.02\n",
      " #       x = x.dropna(how='any')\n",
      "  #  elif what_to_prioritize is 'samples':\n",
      "   #     for i in range (47):\n",
      "    #        x = x.dropna(axis=1, thresh=(x.shape[0]*threshold_num))\n",
      "     #       x = x.dropna(thresh=(x.shape[1]*threshold_num))\n",
      "      #      threshold_num += 0.02\n",
      "       # x = x.dropna(axis=1, how='any')\n",
      "#    else:\n",
      " #       return(\"Need to specify what to prioritize: 'features' or 'samples'\")\n",
      "  #  return (x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#def read_w2_harmonized (file_name):\n",
      " #   x = pd.read_csv(file_name, na_values=[])\n",
      "  #  x = x.dropna(axis=1, how='all')\n",
      "   # x = x.dropna(how='all')\n",
      "    #x = x[pd.notnull(x['pwv'])]\n",
      "#    x = x[pd.notnull(x['exmBMI'])]\n",
      " #   x = x[pd.notnull(x['exmBPsys_jbs'])]\n",
      "  #  x = x[pd.notnull(x['exmWaist'])]\n",
      "   # x = x[pd.notnull(x['labsColesterolo'])]\n",
      "    #x = x[pd.notnull(x['vasPSV'])]\n",
      "#    x = x[pd.notnull(x['vasIMT'])]\n",
      " #   x = x[pd.notnull(x['vasvti'])]\n",
      "  #  y, z = x.shape\n",
      "   # x = x.dropna(axis=1, thresh=(y/3))\n",
      "    #y, z = x.shape\n",
      "#    x = x.dropna(thresh=(z/2))\n",
      " #   y, z = x.shape\n",
      "  #  x = x.dropna(axis=1, thresh=(y/2))\n",
      "   # y, z = x.shape\n",
      "    #x = x.dropna(thresh=(z*4/5))\n",
      "#    y, z = x.shape\n",
      " #   x = x.dropna(axis=1, thresh=(y*9/10))\n",
      "  #  y, z = x.shape\n",
      "   # x = x.dropna(thresh=(z*9/10))\n",
      "    #y, z = x.shape\n",
      "#    x = x.dropna(axis=1, thresh=(y*99/100))\n",
      " #   y, z = x.shape\n",
      "  #  x = x.dropna(how='any')\n",
      "   # return (x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean_data(df):\n",
      "    drop_cols = [\n",
      "        'id_sir', 'id_mad', 'pwvDate', 'FirstVisitDate', 'SecondVisitDate', 'ThirdVisitDate',\n",
      "        'FourthVisitDate', 'Birthdate', 'Scandate', 'date_neo', 'Visit', 'Wave', 'Subject#',\n",
      "        'Subject_ID[4%]', 'Subject_ID[38%]', 'Subject_ID[66%]', 'Occupation', 'Education',\n",
      "        'MaritalStatus',\n",
      "        'SpirometerPrELA','SpirometerELA',\n",
      "    ]\n",
      "    \n",
      "    for col in drop_cols:\n",
      "        try:\n",
      "            df = df.drop(col,1)\n",
      "        except:\n",
      "            pass\n",
      "    return (df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def convert_data(df):\n",
      "    data = df.as_matrix()\n",
      "    col_names = df.columns.values\n",
      "    return (data, col_names, df.columns.get_loc(\"Age\"), df.columns.get_loc(\"id_individual\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean_convert(df):\n",
      "    return (convert_data(clean_data(df)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This bin_data relies/assumes that data_matrix is sorted by age!\n",
      "def bin_data_OLD(start, end, size, data_matrix, age_col):\n",
      "    bins = range (start, end+size, size)\n",
      "    counts,bins = np.histogram(age_col,bins=bins)\n",
      "    views=[]\n",
      "    index=0\n",
      "    for i in range (0,len(counts)):\n",
      "        views.append (data_matrix[index : index+counts[i],:])\n",
      "        index = index+counts[i]\n",
      "    views.pop()\n",
      "    # initialize the class_values array (center of each bin)\n",
      "    class_vals = [x+(size/2.0) for x in range(start, end, size)]\n",
      "    class_vals.pop()\n",
      "    return (views, class_vals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bin_data(start, end, size, data_matrix, age_col):\n",
      "    bins = range (start, end+size, size)\n",
      "    digitized = np.digitize(age_col, bins)\n",
      "\n",
      "    views=[]\n",
      "    for i in range (1,len(bins)):\n",
      "        views.append (data_matrix[digitized == i,:])\n",
      "#    views.pop()\n",
      "    # initialize the class_values array (center of each bin)\n",
      "    class_vals = [x+(size/2.0) for x in range(start, end, size)]\n",
      "#    class_vals.pop()\n",
      "    return (views, class_vals)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def normalize_by_columns ( full_stack, mins = None, maxs = None ):\n",
      "    \"\"\"This is a global function to normalize a matrix by columns.\n",
      "    If numpy 1D arrays of mins and maxs are provided, the matrix will be normalized against these ranges\n",
      "    Otherwise, the mins and maxs will be determined from the matrix, and the matrix will be normalized\n",
      "    against itself. The mins and maxs will be returned as a tuple.\n",
      "    Out of range matrix values will be clipped to min and max (including +/- INF)\n",
      "    zero-range columns will be set to 0.\n",
      "    NANs in the columns will be set to 0.\n",
      "    The normalized output range is hard-coded to 0-100\n",
      "    \"\"\"\n",
      "    # Edge cases to deal with:\n",
      "    # Range determination:\n",
      "    # 1. features that are nan, inf, -inf\n",
      "    # max and min determination must ignore invalid numbers\n",
      "    # nan -> 0, inf -> max, -inf -> min\n",
      "    # Normalization:\n",
      "    # 2. feature values outside of range\n",
      "    # values clipped to range (-inf to min -> min, max to inf -> max) - leaves nan as nan\n",
      "    # 3. feature ranges that are 0 result in nan feature values\n",
      "    # 4. all nan feature values set to 0\n",
      "\n",
      "    # Turn off numpy warnings, since we're taking care of invalid values explicitly\n",
      "    oldsettings = np.seterr(all='ignore')\n",
      "    if (mins is None or maxs is None):\n",
      "        # mask out NANs and +/-INFs to compute min/max\n",
      "        full_stack_m = np.ma.masked_invalid (full_stack, copy=False)\n",
      "        maxs = full_stack_m.max (axis=0)\n",
      "        mins = full_stack_m.min (axis=0)\n",
      "\n",
      "    # clip the values to the min-max range (NANs are left, but +/- INFs are taken care of)\n",
      "    full_stack.clip (mins, maxs, full_stack)\n",
      "    # remake a mask to account for NANs and divide-by-zero from max == min\n",
      "    full_stack_m = np.ma.masked_invalid (full_stack, copy=False)\n",
      "\n",
      "    # Normalize\n",
      "    full_stack_m -= mins\n",
      "    full_stack_m /= (maxs - mins)\n",
      "    # Left over NANs and divide-by-zero from max == min become 0\n",
      "    # Note the deep copy to change the numpy parameter in-place.\n",
      "    full_stack[:] = full_stack_m.filled (0) * 100.0\n",
      "\n",
      "    # return settings to original\n",
      "    np.seterr(**oldsettings)\n",
      "\n",
      "    return (mins,maxs)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Feature Selection"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Fisher"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Fisher(split):\n",
      "    \"\"\"Takes a FeatureSet_Discrete as input and calculates a Fisher score for\n",
      "    each feature. Returns a newly instantiated instance of FisherFeatureWeights.\n",
      "\n",
      "    For:\n",
      "    N = number of classes\n",
      "    F = number of features\n",
      "    It = total number of images in training set\n",
      "    Ic = number of images in a given class\n",
      "    \"\"\"\n",
      "\n",
      "    if split == None:\n",
      "        import inspect\n",
      "        form_str = 'You passed in a None as a training set to the function {0}.{1}'\t\n",
      "        raise ValueError( form_str.format( cls.__name__, inspect.stack()[1][3] ) )\n",
      "\n",
      "    # we deal with NANs/INFs separately, so turn off numpy warnings about invalid floats.\n",
      "    oldsettings = np.seterr(all='ignore')\n",
      "\n",
      "    def get_train_3d (self):\n",
      "        if self.train_3d is None:\n",
      "            self.train_3d = get_class_mat_list (self.train_set, self.train_classed_labels)\n",
      "        return (self.train_3d)\n",
      "\n",
      "    def get_test_3d (self):\n",
      "        if self.test_3d is None:\n",
      "            self.test_3d = get_class_mat_list (self.test_set, self.test_classed_labels)\n",
      "        return (self.test_3d)\n",
      "\n",
      "    #class_mats = split.get_train_3d()\n",
      "    class_mats = get_class_mat_list (split.train_set, split.train_classed_labels)\n",
      "    # 1D matrix 1 * F\n",
      "    population_means = np.mean( split.train_set, axis = 0 )\n",
      "    n_classes = class_mats.shape[0]\n",
      "    n_features = split.train_set.shape[1]\n",
      "\n",
      "    # 2D matrix shape N * F\n",
      "    intra_class_means = np.empty( [n_classes, n_features] )\n",
      "    # 2D matrix shape N * F\n",
      "    intra_class_variances = np.empty( [n_classes, n_features] )\n",
      "\n",
      "    class_index = 0\n",
      "    for class_feature_matrix in class_mats:\n",
      "        intra_class_means[ class_index ] = np.mean( class_feature_matrix, axis=0 )\n",
      "    # Note that by default, numpy divides by N instead of the more common N-1, hence ddof=1.\n",
      "        intra_class_variances[ class_index ] = np.var( class_feature_matrix, axis=0, ddof=1 )\n",
      "        class_index += 1\n",
      "\n",
      "    # 1D matrix 1 * F\n",
      "    # we deal with NANs/INFs separately, so turn off numpy warnings about invalid floats.\n",
      "    # for the record, in numpy:\n",
      "    # 1./0. = inf, 0./inf = 0., 1./inf = 0. inf/0. = inf, inf/inf = nan\n",
      "    # 0./0. = nan, nan/0. = nan, 0/nan = nan, nan/nan = nan, nan/inf = nan, inf/nan = nan\n",
      "    # We can't deal with NANs only, must also deal with pos/neg infs\n",
      "    # The masked array allows for dealing with \"invalid\" floats, which includes nan and +/-inf\n",
      "    denom = np.mean( intra_class_variances, axis = 0 )\n",
      "    denom[denom == 0] = np.nan\n",
      "    feature_weights_m = np.ma.masked_invalid (\n",
      "            ( np.square( population_means - intra_class_means ).sum( axis = 0 ) /\n",
      "        (n_classes - 1) ) / denom\n",
      "        )\n",
      "    # return numpy error settings to original\n",
      "    np.seterr(**oldsettings)\n",
      "\n",
      "    # the filled(0) method of the masked array sets all nan and infs to 0\n",
      "    fisher_values = feature_weights_m.filled(0).tolist()\n",
      "\n",
      "    return (fisher_values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Pearson"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Pearson(split):\n",
      "    \"\"\"Calculate regression parameters and correlation statistics that fully define\n",
      "    a continuous classifier.\n",
      "\n",
      "    At present the feature weights are proportional the Pearson correlation coefficient\n",
      "    for each given feature.\"\"\"\n",
      "\n",
      "    from scipy import stats\n",
      "\n",
      "    # Known issue: running stats.linregress() with np.seterr (all='raise') has caused\n",
      "    # arithmetic underflow (FloatingPointError: 'underflow encountered in stdtr' )\n",
      "    # I think this is something we can safely ignore in this function, and return settings\n",
      "    # back to normal at the end. -CEC\n",
      "    np.seterr (under='ignore')\n",
      "\n",
      "    matrix = split.train_set\n",
      "    #FIXME: maybe add some dummyproofing to constrain incoming array size\n",
      "\n",
      "    #r_val_sum = 0\n",
      "    r_val_squared_sum = 0\n",
      "    #r_val_cubed_sum = 0\n",
      "    \n",
      "    ages = split.train_labels\n",
      "\n",
      "    ground_truths = np.array( [float(val) for val in ages] )\n",
      "    pearson_coeffs = np.zeros(matrix.shape[1])\n",
      "\n",
      "    for feature_index in range( matrix.shape[1] ):\n",
      "        slope, intercept, pearson_coeff, p_value, std_err = stats.linregress(\n",
      "            ground_truths, matrix[:,feature_index]\n",
      "        )\n",
      "\n",
      "        pearson_coeffs[feature_index] = pearson_coeff\n",
      "        r_val_squared_sum += pearson_coeff * pearson_coeff\n",
      "\n",
      "# We're just returning the pearsons^2 now...\n",
      "#    pearson_values = [val*val / r_val_squared_sum for val in pearson_coeffs ]\n",
      "#    pearson_coeffs = (pearson_coeffs * pearson_coeffs) / r_val_squared_sum\n",
      "    pearson_coeffs *= pearson_coeffs\n",
      "    \n",
      "\n",
      "    # Reset numpy\n",
      "    np.seterr (all='raise')\n",
      "\n",
      "    return pearson_coeffs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "PCA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pca(split):\n",
      "    from sklearn.decomposition import PCA\n",
      "    pca = PCA()\n",
      "    new_train = pca.fit_transform(split.get_train_vstack())\n",
      "    new_test = pca.transform(split.get_test_vstack())\n",
      "    return(new_train, new_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "LDA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lda(train, test, split):\n",
      "    lda = LDA()\n",
      "    lda_train = lda.fit(train, split.train_classed_labels).transform(train)\n",
      "    lda_test = lda.transform(test)\n",
      "    return (lda_train, lda_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "mRMR"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mrmr(split, **kwargs):\n",
      "    import subprocess\n",
      "    import tempfile\n",
      "    import os\n",
      "    import time\n",
      "    \n",
      "    if 'thresh' in kwargs:\n",
      "        thresh=kwargs['thresh']\n",
      "    else:\n",
      "        thresh=0.1\n",
      "    if 'sigfigs' in kwargs:\n",
      "        sigfigs=kwargs['sigfigs']\n",
      "    else:\n",
      "        sigfigs=7\n",
      "\n",
      "    weights = []  \n",
      "    class_labels = split.train_classed_labels.reshape(len(split.train_classed_labels),1)\n",
      "    names = [float(i) for i in range (0,(split.train_set).shape[1]+1)]\n",
      "    data = np.append(class_labels, split.train_set, axis=1)\n",
      "    data = [names, data]\n",
      "    data = np.vstack(data)\n",
      "    tmpfile = tempfile.NamedTemporaryFile(delete=False).name\n",
      "    # only save 5 sig-figs in text file\n",
      "    np.savetxt(tmpfile, data, fmt='%.{}g'.format(sigfigs), delimiter=\",\")\n",
      "\n",
      "    ignore_lines = True\n",
      "    cmd_list = [Config.mrmr_bin, \"-i\",tmpfile, \"-n\", str(split.train_set.shape[1]),\n",
      "                                \"-s\", str(split.train_set.shape[0]), '-t', str(thresh)]\n",
      "    cmd_str = subprocess.Popen(cmd_list, stdout=subprocess.PIPE).stdout\n",
      "#    cmd_str = subprocess.Popen([Config.mrmr_bin, \"-i\",\"/home/suned/mrmr/test_lung_s3.csv\"], stdout=subprocess.PIPE).stdout\n",
      "#    t0 = time.time()\n",
      "    for line in cmd_str:\n",
      "#        print (\"time: \"+str(time.time() - t0))\n",
      "#        t0 = time.time()\n",
      "        line = line.decode(\"utf-8\").strip()\n",
      "#        print (line)\n",
      "        # Ignore everything until lines like this:\n",
      "        #    *** mRMR features *** \n",
      "        # Order \t Fea \t Name \t Score\n",
      "        if '*** mRMR features ***' in line:\n",
      "            ignore_lines = False\n",
      "            continue\n",
      "        if ignore_lines:\n",
      "                continue\n",
      "        if 'Order' in line:\n",
      "                continue\n",
      "        cols = line.split()\n",
      "        if (len(cols) != 4):\n",
      "                continue\n",
      "        try:\n",
      "            cols = [int(cols[0]), int(cols[1]), float(cols[3])]\n",
      "        except:\n",
      "            continue\n",
      "        weights.append(cols)\n",
      "    if len (weights) > 0:\n",
      "        weights = np.vstack(weights)\n",
      "        weights = weights[weights[:,1].argsort()]\n",
      "        weights = weights[:,2]\n",
      "        os.unlink (tmpfile)\n",
      "    else:\n",
      "        raise ValueError (\" \".join(cmd_list)+\"\\mrmr returned no weights\")\n",
      "    return weights"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_mrmr(wavenum, splitnum):\n",
      "    mrmr_filename_base = Config.wave_info(wavenum).filename_base\n",
      "    mrmr_filename_base += Config.mrmr_sfx\n",
      "    mrmr_file_name = mrmr_filename_base+Config.split_num_fmt.format(splitnum)+\".pickle\"\n",
      "    mrmr_file_path = os.path.join(Config.data_dir(),mrmr_filename_base,mrmr_file_name)\n",
      "    with open(mrmr_file_path, 'rb') as f:\n",
      "        mrmr_weights = pickle.load(f)\n",
      "    return (mrmr_weights)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Classification"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# map used to get model info by name.\n",
      "# Names are populated where the functions are defined.\n",
      "ModelInfoByModelName = {}\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "WND"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def marg_prob_to_pred_value (marg_probs, class_vals):\n",
      "    weighted = np.array(marg_probs)*np.array(class_vals)\n",
      "    return (np.sum(weighted))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def WND5(train_classed_data, test_classed_data, classnames_list, split):\n",
      "    \"\"\"\n",
      "    Don't call this function directly, use the wrapper functions\n",
      "    DiscreteBatchClassificationResult.New() (for test sets) or\n",
      "    DiscreteImageClassificationResult.NewWND5() (for single images)\n",
      "    Both of these functions have dummyproofing.\n",
      "\n",
      "    If you're using this function, your training set data is not continuous\n",
      "    for N images and M features:\n",
      "    trainingset is list of length L of N x M numpy matrices\n",
      "    testtile is a 1 x M list of feature values\n",
      "    NOTE: the trainingset and test image must have the same number of features!!!\n",
      "    AND: the features must be in the same order!!\n",
      "    Returns an instance of the class DiscreteImageClassificationResult\n",
      "    FIXME: what about tiling??\n",
      "    \"\"\"\n",
      "    n_test_samples = test_classed_data.shape[0]\n",
      "    n_train_samples = train_classed_data.shape[0]\n",
      "    predicted_classes = np.zeros(n_test_samples)\n",
      "    predicted_values = np.zeros(n_test_samples)\n",
      "    \n",
      "    epsilon = np.finfo( np.float ).eps\n",
      "    testimg_idx = 0\n",
      "    trainimg_idx = 0\n",
      "    \n",
      "    for testimg_idx in range( n_test_samples ):\n",
      "        test_class_label = split.test_classed_labels[testimg_idx]\n",
      "\n",
      "        # initialize\n",
      "        class_dists = {}\n",
      "        class_counts = {}\n",
      "\n",
      "        for trainimg_idx in range( n_train_samples ):\n",
      "            train_class_label = split.train_classed_labels[trainimg_idx]\n",
      "            if not train_class_label in class_dists:\n",
      "                class_dists [train_class_label] = 0.0\n",
      "                class_counts[train_class_label] = 0.0\n",
      "\n",
      "            dists = np.absolute (train_classed_data [trainimg_idx] - test_classed_data [testimg_idx])\n",
      "            w_dist = np.sum( dists )\n",
      "            if w_dist > epsilon:\n",
      "                class_counts[train_class_label] += 1.0\n",
      "            else:\n",
      "                continue\n",
      "\n",
      "            w_dist = np.sum( np.square( dists ) )\n",
      "            # The exponent -5 is the \"5\" in \"WND5\"\n",
      "            class_dists[ train_class_label ] += w_dist ** -5\n",
      "\n",
      "        \n",
      "        class_idx = 0\n",
      "        class_similarities = [0]*len(class_dists)\n",
      "        for class_label in classnames_list:\n",
      "            class_similarities[class_idx] = class_dists[class_label] / class_counts[class_label]\n",
      "            class_idx += 1\n",
      "\n",
      "        norm_factor = sum( class_similarities )\n",
      "        marg_probs = np.array( [ x / norm_factor for x in class_similarities ] )\n",
      "\n",
      "        predicted_class_idx = marg_probs.argmax()\n",
      "\n",
      "        predicted_classes[testimg_idx] = classnames_list[ predicted_class_idx ]\n",
      "        predicted_values[testimg_idx] = marg_prob_to_pred_value (marg_probs, classnames_list)\n",
      "\n",
      "    return (predicted_classes, predicted_values, split.test_labels)\n",
      "\n",
      "ModelInfoByModelName['WND5'] = [WND5,'C']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "SVC"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def svc(train_classed_data, test_classed_data, classnames_list, split):\n",
      "    from sklearn.svm import SVC\n",
      "    clf = SVC(kernel='linear', probability=True)\n",
      "    clf.fit(train_classed_data, split.train_classed_labels)\n",
      "    predicted_classes = clf.predict(test_classed_data)\n",
      "    predicted_values = np.zeros(test_classed_data.shape[0])\n",
      "    testimg_idx = 0\n",
      "    for sample in test_classed_data:\n",
      "        predicted_values[testimg_idx] = marg_prob_to_pred_value (clf.predict_proba(sample), classnames_list)\n",
      "        testimg_idx += 1\n",
      "    actual = split.test_labels\n",
      "    return (predicted_classes, predicted_values, actual)\n",
      "\n",
      "ModelInfoByModelName['SVC'] = [svc,'C']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "NuSVC"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def nu_svc(train_classed_data, test_classed_data, classnames_list, split):\n",
      "    from sklearn.svm import NuSVC\n",
      "    clf = NuSVC(nu = 0.3, kernel='linear', probability=True)\n",
      "    clf.fit(train_classed_data, split.train_classed_labels)\n",
      "    predicted_classes = clf.predict(test_classed_data)\n",
      "    predicted_values = np.zeros(test_classed_data.shape[0])\n",
      "    testimg_idx = 0\n",
      "    for sample in test_classed_data:\n",
      "        predicted_values[testimg_idx] = marg_prob_to_pred_value (clf.predict_proba(sample), classnames_list)\n",
      "        testimg_idx += 1\n",
      "    actual = split.test_labels\n",
      "    return (predicted_classes, predicted_values, actual)\n",
      "\n",
      "ModelInfoByModelName['NuSVC'] = [nu_svc,'C']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Gradient Boosting Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grad_clf(train_classed_data, test_classed_data, classnames_list, split):\n",
      "    from sklearn.ensemble import GradientBoostingClassifier\n",
      "    clf = GradientBoostingClassifier(n_estimators=100)\n",
      "    clf.fit(train_classed_data, split.train_classed_labels)\n",
      "    predicted_classes = clf.predict(test_classed_data)\n",
      "    predicted_values = np.zeros(test_classed_data.shape[0])\n",
      "    testimg_idx = 0\n",
      "    for sample in test_classed_data:\n",
      "        predicted_values[testimg_idx] = marg_prob_to_pred_value (clf.predict_proba(sample), classnames_list)\n",
      "        testimg_idx += 1\n",
      "    actual = split.test_labels\n",
      "    return (predicted_classes, predicted_values, actual)\n",
      "\n",
      "ModelInfoByModelName['GradClf'] = [grad_clf,'C']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Decision Tree Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def dec_tree_clf(train_classed_data, test_classed_data, classnames_list, split):\n",
      "    from sklearn.tree import DecisionTreeClassifier\n",
      "    clf = DecisionTreeClassifier()\n",
      "    clf.fit(train_classed_data, split.train_classed_labels)\n",
      "    predicted_classes = clf.predict(test_classed_data)\n",
      "    predicted_values = np.zeros(test_classed_data.shape[0])\n",
      "    testimg_idx = 0\n",
      "    for sample in test_classed_data:\n",
      "        predicted_values[testimg_idx] = marg_prob_to_pred_value (clf.predict_proba(sample), classnames_list)\n",
      "        testimg_idx += 1\n",
      "    actual = split.test_labels\n",
      "    return (predicted_classes, predicted_values, actual)\n",
      "\n",
      "ModelInfoByModelName['DecTreeClf'] = [dec_tree_clf,'C']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Random Forest Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rand_forest_clf(train_classed_data, test_classed_data, classnames_list, split):\n",
      "    from sklearn.ensemble import RandomForestClassifier\n",
      "    clf = RandomForestClassifier(n_estimators = 30)\n",
      "    clf.fit(train_classed_data, split.train_classed_labels)\n",
      "    predicted_classes = clf.predict(test_classed_data)\n",
      "    predicted_values = np.zeros(test_classed_data.shape[0])\n",
      "    testimg_idx = 0\n",
      "    for sample in test_classed_data:\n",
      "        predicted_values[testimg_idx] = marg_prob_to_pred_value (clf.predict_proba(sample), classnames_list)\n",
      "        testimg_idx += 1\n",
      "    actual = split.test_labels\n",
      "    return (predicted_classes, predicted_values, actual)\n",
      "\n",
      "ModelInfoByModelName['RandForClf'] = [rand_forest_clf,'C']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "K Neighbors Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def k_neigh_clf(train_classed_data, test_classed_data, classnames_list, split):\n",
      "    from sklearn.neighbors import KNeighborsClassifier\n",
      "    clf = KNeighborsClassifier(n_neighbors=10, weights='distance')\n",
      "    clf.fit(train_classed_data, split.train_classed_labels)\n",
      "    predicted_classes = clf.predict(test_classed_data)\n",
      "    predicted_values = np.zeros(test_classed_data.shape[0])\n",
      "    testimg_idx = 0\n",
      "    for sample in test_classed_data:\n",
      "        predicted_values[testimg_idx] = marg_prob_to_pred_value (clf.predict_proba(sample), classnames_list)\n",
      "        testimg_idx += 1\n",
      "    actual = split.test_labels\n",
      "    return (predicted_classes, predicted_values, actual)\n",
      "\n",
      "ModelInfoByModelName['KNeighClf'] = [k_neigh_clf,'C']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "AdaBoost Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ada_clf(train_classed_data, test_classed_data, classnames_list, split):\n",
      "    from sklearn.ensemble import AdaBoostClassifier\n",
      "    clf = AdaBoostClassifier(n_estimators=50)\n",
      "    clf.fit(train_classed_data, split.train_classed_labels)\n",
      "    predicted_classes = clf.predict(test_classed_data)\n",
      "    predicted_values = np.zeros(test_classed_data.shape[0])\n",
      "    testimg_idx = 0\n",
      "    for sample in test_classed_data:\n",
      "        predicted_values[testimg_idx] = marg_prob_to_pred_value (clf.predict_proba(sample), classnames_list)\n",
      "        testimg_idx += 1\n",
      "    actual = split.test_labels\n",
      "    return (predicted_classes, predicted_values, actual)\n",
      "\n",
      "ModelInfoByModelName['AdaBstClf'] = [ada_clf,'C']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Regression"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Lasso Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lasso_regression(train_data, train_labels, test_data, test_labels):\n",
      "    from sklearn import linear_model \n",
      "    from sklearn.linear_model import Lasso\n",
      "    regr = linear_model.Lasso\n",
      "    clf=regr()\n",
      "    clf.fit(train_data, train_labels)\n",
      "    predicted = clf.predict(test_data)\n",
      "    actual = test_labels\n",
      "    return (predicted, actual)\n",
      "\n",
      "ModelInfoByModelName['LassoReg'] = [lasso_regression,'R']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Linear Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lin_reg(train_data, train_labels, test_data, test_labels):\n",
      "    from sklearn.linear_model import LinearRegression\n",
      "    lin_reg = LinearRegression()\n",
      "    lin_reg.fit(train_data, train_labels)\n",
      "    predicted = lin_reg.predict(test_data)\n",
      "    actual = test_labels\n",
      "    return (predicted, actual)\n",
      "\n",
      "ModelInfoByModelName['LinReg'] = [lin_reg,'R']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "SVR"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def svr(train_data, train_labels, test_data, test_labels):\n",
      "    from sklearn.svm import SVR\n",
      "    svr = SVR('linear')\n",
      "    svr.fit(train_data, train_labels)\n",
      "    predicted = svr.predict(test_data)\n",
      "    actual = test_labels\n",
      "    return (predicted, actual)\n",
      "\n",
      "ModelInfoByModelName['SVR'] = [svr,'R']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "NuSVR"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def nusvr(train_data, train_labels, test_data, test_labels):\n",
      "    from sklearn.svm import NuSVR\n",
      "    nusvr = NuSVR('rbf')\n",
      "    nusvr.fit(train_data, train_labels)\n",
      "    predicted = svr.predict(test_data)\n",
      "    actual = test_labels\n",
      "    return (predicted, actual)\n",
      "\n",
      "ModelInfoByModelName['NuSVR'] = [nusvr,'R']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Ridge Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ridge(train_data, train_labels, test_data, test_labels, solver):\n",
      "    from sklearn.linear_model import Ridge\n",
      "    ridge = Ridge(alpha=0.1, solver=solver)\n",
      "    ridge.fit(train_data, train_labels)\n",
      "    predicted = ridge.predict(test_data)\n",
      "    actual = test_labels\n",
      "    return (predicted, actual)\n",
      "\n",
      "ModelInfoByModelName['RidgeReg'] = [ridge,'R']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Elastic Net"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def e_net(train_data, train_labels, test_data, test_labels):\n",
      "    from sklearn.linear_model import ElasticNet\n",
      "    e_net = ElasticNet(alpha=0.001, l1_ratio=0.5,)\n",
      "    e_net.fit(train_data, train_labels)\n",
      "    predicted = e_net.predict(test_data)\n",
      "    actual = test_labels\n",
      "    return (predicted, actual)\n",
      "\n",
      "ModelInfoByModelName['ElasNetReg'] = [e_net,'R']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Decision Tree Regressor"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def dec_tree(train_data, train_labels, test_data, test_labels):\n",
      "    from sklearn.tree import DecisionTreeRegressor\n",
      "    tree = DecisionTreeRegressor()\n",
      "    tree.fit(train_data, train_labels)\n",
      "    predicted = tree.predict(test_data)\n",
      "    actual = test_labels\n",
      "    return (predicted, actual)\n",
      "\n",
      "ModelInfoByModelName['DecTreeReg'] = [dec_tree,'R']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Random Forest Regressor"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rand_forest(train_data, train_labels, test_data, test_labels):\n",
      "    from sklearn.ensemble import RandomForestRegressor\n",
      "    forest = RandomForestRegressor(n_estimators=30)\n",
      "    forest.fit(train_data, train_labels)\n",
      "    predicted = forest.predict(test_data)\n",
      "    actual = test_labels\n",
      "    return (predicted, actual)\n",
      "\n",
      "ModelInfoByModelName['RandForReg'] = [rand_forest,'R']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "KNeighbors Regressor"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def k_neigh(train_data, train_labels, test_data, test_labels):\n",
      "    from sklearn.neighbors import KNeighborsRegressor\n",
      "    neigh = KNeighborsRegressor(n_neighbors = 20, weights='distance', p=1)\n",
      "    neigh.fit(train_data, train_labels)\n",
      "    predicted = neigh.predict(test_data)\n",
      "    actual = test_labels\n",
      "    return (predicted, actual)\n",
      "\n",
      "ModelInfoByModelName['KNeighReg'] = [k_neigh,'R']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Gradient Boosting Regressor"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def grad_boost(train_data, train_labels, test_data, test_labels):\n",
      "    from sklearn.ensemble import GradientBoostingRegressor\n",
      "    boost = GradientBoostingRegressor(n_estimators = 120)\n",
      "    boost.fit(train_data, train_labels)\n",
      "    predicted = boost.predict(test_data)\n",
      "    actual = test_labels\n",
      "    return (predicted, actual)\n",
      "\n",
      "ModelInfoByModelName['GraBstReg'] = [grad_boost,'R']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "SGD Regressor"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sgd_reg(train_data, train_labels, test_data, test_labels):\n",
      "    from sklearn.linear_model import SGDRegressor\n",
      "    sgd = SGDRegressor(penalty='elasticnet', n_iter=20)\n",
      "    sgd.fit(train_data, train_labels)\n",
      "    predicted = sgd.predict(test_data)\n",
      "    actual = test_labels\n",
      "    return (predicted, actual)\n",
      "\n",
      "ModelInfoByModelName['SGDReg'] = [sgd_reg,'R']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "AdaBoost (KNeigh) Regressor"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ada_boost_reg(train_data, train_labels, test_data, test_labels):\n",
      "    from sklearn.ensemble import AdaBoostRegressor\n",
      "    from sklearn.neighbors import KNeighborsRegressor\n",
      "    ada_boost = AdaBoostRegressor(KNeighborsRegressor(), n_estimators = 10, loss='linear')\n",
      "    ada_boost.fit(train_data, train_labels)\n",
      "    predicted = ada_boost.predict(test_data)\n",
      "    actual = test_labels\n",
      "    return (predicted, actual)\n",
      "\n",
      "ModelInfoByModelName['AdaBstReg'] = [ada_boost_reg,'R']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Maps to convert between model names and model functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ModelByModelName = {k:v[0] for k, v in ModelInfoByModelName.items()}\n",
      "ModelNameByModel = {v[0]:k for k, v in ModelInfoByModelName.items()}\n",
      "ModelTypeByModel = {v[0]:v[1] for k, v in ModelInfoByModelName.items()}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Model Scoring"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Accuracy"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Average Accuracy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Haven't ever used\n",
      "# but will calculate accuracy from predicted class names and actual classed labels\n",
      "def Accuracy(predicted_classes, actual):\n",
      "    acc = []\n",
      "    right_wrong = predicted_classes - actual\n",
      "    for prediction in right_wrong:\n",
      "        if prediction == 0:\n",
      "            acc = np.append(acc, 1)\n",
      "        else:\n",
      "            acc = np.append(acc, 0)\n",
      "    acc = np.mean(acc)\n",
      "    return (acc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Per Class Accuracy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Gives accuracy for each class\n",
      "def Class_Acc(predictions, actual, num_classes, test_size):\n",
      "    acc = []\n",
      "    right_wrong = predictions - actual\n",
      "    for prediction in right_wrong:\n",
      "        if prediction == 0:\n",
      "            acc = np.append(acc, 1)\n",
      "        else:\n",
      "            acc = np.append(acc, 0)\n",
      "    class_acc = []\n",
      "    index=0\n",
      "    for i in range (0,num_classes):\n",
      "        class_acc.append (acc[index : index+test_size])\n",
      "        print (np.mean(acc[index : index+test_size]))\n",
      "        index = index+test_size\n",
      "    class_acc = np.mean(class_acc)\n",
      "    print ('Average Accuracy')\n",
      "    print (class_acc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Performance VS Feature Number"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Score_Reg(model, num_features, split):\n",
      "    scores = []\n",
      "    lda_scores = []\n",
      "    n_classes = split.get_n_classes()\n",
      "    for num_feature in num_features:\n",
      "        new_train = split.train_set[:,:num_feature]\n",
      "        new_test = split.test_set[:,:num_feature]\n",
      "        predictions, actual = model(new_train, split.train_labels, new_test, split.test_labels)\n",
      "        score, p_value = pearsonr(predictions, actual)\n",
      "        scores = np.append(scores, score)\n",
      "        if num_feature>n_classes-2:\n",
      "            try:\n",
      "                lda_train, lda_test = lda(new_train, new_test, split)\n",
      "                lda_predictions, lda_actual = model(lda_train, split.train_labels, lda_test, split.test_labels)\n",
      "                lda_score, lda_p_value = pearsonr(lda_predictions, lda_actual)\n",
      "                lda_scores = np.append(lda_scores,lda_score)\n",
      "            except np.linalg.LinAlgError:\n",
      "                pass\n",
      "    scores_R = scores*scores\n",
      "    lda_scores_R = lda_scores*lda_scores\n",
      "    return(scores_R, lda_scores_R)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Score_Clf(model, num_features, class_vals, split):\n",
      "    scores = []\n",
      "    lda_scores = []\n",
      "    n_classes = split.get_n_classes()\n",
      "    for num_feature in num_features:\n",
      "        new_train = split.train_set[:,:num_feature]\n",
      "        new_test = split.test_set[:,:num_feature]\n",
      "        pred_cls_names, predictions, actual = model(new_train, new_test, class_vals, split)\n",
      "        score, p_value = pearsonr(predictions, actual)\n",
      "        scores = np.append(scores, score)\n",
      "        if num_feature>n_classes-2:\n",
      "            try:\n",
      "                lda_train, lda_test = lda(new_train, new_test, split)\n",
      "                lda_pred_cls_names, lda_predictions, lda_actual = model(lda_train, lda_test, class_vals, split)\n",
      "                lda_score, lda_p_value = pearsonr(lda_predictions, lda_actual)\n",
      "                lda_scores = np.append(lda_scores,lda_score)\n",
      "            except np.linalg.LinAlgError:\n",
      "                pass\n",
      "    scores_R = scores*scores\n",
      "    lda_scores_R = lda_scores*lda_scores\n",
      "    return(scores_R, lda_scores_R)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Run_Model (model, train, test, split):\n",
      "    if ModelTypeByModel[model] == 'C':\n",
      "        cls_names, predictions, actual = model (train, test, split.get_class_vals(), split)\n",
      "    else:\n",
      "        predictions, actual = model (train, split.train_labels, test, split.test_labels)\n",
      "    return (predictions, actual)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Varies number of features, reports R^2 for model and model+LDA\n",
      "# N.B.: Assumes features are already sorted by weight!\n",
      "def Score_Model(model, feature_range, split):\n",
      "    scores = np.array([None]*len(feature_range), dtype=np.float64)\n",
      "    lda_scores = np.array([None]*len(feature_range), dtype=np.float64)\n",
      "    idx = 0\n",
      "    n_classes = split.get_n_classes()\n",
      "    class_vals = split.get_class_vals()\n",
      "    for feature_num in feature_range:\n",
      "        new_train, new_test = split.get_trimmed_features (feature_num)\n",
      "        predictions, actual = Run_Model (model, new_train, new_test, split)\n",
      "\n",
      "        score, p_value = pearsonr(predictions, actual)\n",
      "        scores[idx] = score*score\n",
      "        if feature_num>n_classes-2:\n",
      "            try:\n",
      "                lda_train, lda_test = lda(new_train, new_test, split)\n",
      "                lda_predictions, lda_actual = Run_Model (model, lda_train, lda_test, split)\n",
      "\n",
      "                lda_score, lda_p_value = pearsonr(lda_predictions, lda_actual)\n",
      "                lda_scores[idx] = lda_score*lda_score\n",
      "            except np.linalg.LinAlgError:\n",
      "                pass\n",
      "        idx += 1\n",
      "\n",
      "    return(scores, lda_scores)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def most_common(lst):\n",
      "    return max(set(lst), key=lst.count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Average_Score (scores):\n",
      "    return (np.nanmean(np.vstack(scores), axis=0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Saves right into home directory\n",
      "def Save_Results(model_name, fs_name, avg_score):\n",
      "    scores_name_base = model_name\n",
      "    out_name = scores_name_base+fs_name\n",
      "    if not os.path.isdir(scores_name_base):\n",
      "        os.mkdir (scores_name_base)\n",
      "    with open(os.path.join(scores_name_base, out_name), 'wb') as name:\n",
      "        np.savetxt(name, avg_score, delimiter=\",\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Saves into home/suned/Graphs/Scores/\n",
      "def Save_Results_New(model_name, fs_name, avg_score):\n",
      "    scores_name_base = 'Graphs/Scores/'+model_name\n",
      "    out_name = model_name+fs_name\n",
      "    if not os.path.isdir(scores_name_base):\n",
      "        os.mkdir (scores_name_base)\n",
      "    with open(os.path.join(scores_name_base, out_name), 'wb') as name:\n",
      "        np.savetxt(name, avg_score, delimiter=\",\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plots graph of num_features and scores\n",
      "# Saves to home/suned/Graphs/Plots/\n",
      "def Graph(title, num_features, avg_score, lda_avg_score):\n",
      "    raw_score = (avg_score[len(avg_score)-1])\n",
      "    num_feat_lda = num_features[len(num_features)-len(lda_avg_score):]\n",
      "    pl.figure()\n",
      "    plot_score, = pl.plot(num_features, avg_score, 'b', label=\"Without LDA\")\n",
      "    plot_lda_score, = pl.plot(num_feat_lda, lda_avg_score, 'g', label=\"With LDA (12 components)\")\n",
      "    plot_benchmark, = pl.plot(num_features, raw_score*np.ones(len(num_features)),'r', label=\"All Features\")\n",
      "\n",
      "    best_num_feat = num_features[np.nanargmax (avg_score)]\n",
      "    best_num_feat_lda = num_feat_lda[np.nanargmax (lda_avg_score)]\n",
      "    \n",
      "   # max_point = pl.plot(best_num_feat, max(avg_score), 'bo', label=\"(Features, Max Score)\")\n",
      "  #  lda_max_point = pl.plot(best_num_feat_lda, max(lda_avg_score), 'go')\n",
      "    max_coords = '('+str(best_num_feat)+', ' + str(\"%.4f\" % (np.nanmax(avg_score)))+')'\n",
      "    text_x = num_features[0]+(0.22*(num_features[-1]-num_features[0]))\n",
      "    pl.text(text_x,0.52, 'No LDA (features, max score): '+max_coords)\n",
      "    lda_max_coords = '('+str(best_num_feat_lda)+', '+str(\"%.4f\" % (np.nanmax(lda_avg_score)))+')'\n",
      "    pl.text(text_x,0.56, 'LDA (features, max score): '+lda_max_coords)\n",
      "    pl.title(title)\n",
      "    pl.xlabel('Number of Features')\n",
      "    pl.ylabel('Coefficient of Determination '+'( $R^2$)')\n",
      "    pl.ylim([0.5, 1.0])\n",
      "    pl.xlim([num_features[0],num_features[-1]])\n",
      "    #pl.xticks(num_features[1], num_features[-1]+1, 20)\n",
      "    # pl.legend([plot_score, plot_lda_score, plot_benchmark], [\"Without LDA\", \"With LDA (12 components)\", \"All Features\"], bbox_to_anchor=(1.05, 1), loc=2)\n",
      "    pl.savefig(os.path.join (Config.graphs_dir,title+'.pdf'), format='pdf', dpi=150)\n",
      "    pl.show()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Separate Graphing Mechanism for PCA\n",
      "# Graphs and saves the scores\n",
      "def Graph_PCA(title, num_features, avg_score, raw_score):\n",
      "    pl.figure()\n",
      "    plot_score, = pl.plot(num_features, avg_score, 'b')\n",
      "    plot_benchmark, = pl.plot(num_features, raw_score*np.ones(len(num_features)), 'r')\n",
      "    ordered_num_feat = [num_features for (avg_score,num_features) in sorted(zip(avg_score,num_features))]\n",
      "    best_num_feat = ordered_num_feat[-1]\n",
      "    max_coords = '('+str(best_num_feat)+', ' + str(\"%.4f\" % (max(avg_score)))+')'\n",
      "    pl.text(len(num_features)*0.22,0.52, '(features, max score): '+max_coords)\n",
      "    pl.title(title)\n",
      "    pl.xlabel('Number of Principal Components')\n",
      "    pl.ylabel('Coefficient of Determination '+'( $R^2$)')\n",
      "    pl.ylim([0.5, 1.0])\n",
      "    pl.xlim([num_features[0],num_features[-1]])\n",
      " #   pl.legend([plot_score, plot_benchmark], [\"PCA Performance\", \"All Features\"], bbox_to_anchor=(1.05, 1), loc=2)\n",
      "    pl.savefig(os.path.join (Config.graphs_dir,title+'.pdf'), format='pdf', dpi=1000)\n",
      "    pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Calculate_Aging_Rate(prediction, actual):\n",
      "    rate = prediction/actual\n",
      "    return (rate)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Calculate_Relative_Age_Rate(prediction, actual):\n",
      "    age_rate = (prediction - actual)/actual\n",
      "    return (age_rate)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Read_Scores(model_name, fs_name):\n",
      "    try:\n",
      "        scores_name_base = os.path.join (Config.graphs_dir,'Scores',model_name)\n",
      "        out_name = model_name+fs_name\n",
      "        lda_out_name = out_name+'_lda'\n",
      "        scores = np.loadtxt(os.path.join(scores_name_base, out_name), delimiter = \",\")\n",
      "    except:\n",
      "        scores_name_base = os.path.join (Config.graphs_dir,'Scores','SCORES',model_name)\n",
      "        out_name = model_name+fs_name\n",
      "        lda_out_name = out_name+'_lda'\n",
      "        scores = np.loadtxt(os.path.join(scores_name_base, out_name), delimiter = \",\")\n",
      "    try:\n",
      "        lda_scores = np.loadtxt(os.path.join(scores_name_base, lda_out_name), delimiter = \",\")\n",
      "        return (scores, lda_scores)\n",
      "    except:\n",
      "        return (scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Regression strategy based on \"Best Aging Rates\" notebook\n",
      "# Works for classifier and regressor models\n",
      "# When read_splits is False, uses the split number as the random seed.\n",
      "def Create_Aging_Scores (num_splits, model, waves = range (1,4), read_splits = True):\n",
      "    aging_scores = {}\n",
      "    pearsons = {}\n",
      "    # Change to range(1,4) for all 3 waves\n",
      "    steps = (num_splits*len(waves))+len(waves)\n",
      "    step = 0\n",
      "    p = ProgressBar(steps)\n",
      "    for wavenum in waves:\n",
      "        p.animate (step)\n",
      "        step +=1\n",
      "        wi = Config.wave_info (wavenum)\n",
      "        wd = Config.wave_data (wavenum)\n",
      "        Split.filename_base = wi.filename_base\n",
      "        best_num_feat = wi.best_models[ModelNameByModel[model]]\n",
      "        for split_num in range(num_splits):\n",
      "            p.animate (step)\n",
      "            step +=1\n",
      "            split = Split()\n",
      "            if (read_splits):\n",
      "                split.read(split.get_fname (split_num))\n",
      "                mrmr_weights = read_mrmr(wavenum, split_num)\n",
      "            else:\n",
      "                # The last optional parameter is a one-time random number seed for the permute function\n",
      "                # The mrmrs were computed for splits where the random number seed was the split number\n",
      "                split.load_wave_train_test_RS (wavenum, split_num)\n",
      "                mrmr_weights = read_mrmr(wavenum, split_num)\n",
      "\n",
      "            split.norm_weigh_sort(mrmr_weights)\n",
      "            new_train, new_test = split.get_trimmed_features (best_num_feat)\n",
      "\n",
      "            try:\n",
      "                lda_train, lda_test = lda(new_train, new_test, split)\n",
      "                predictions, actual = Run_Model (model, lda_train, lda_test, split)\n",
      "\n",
      "                n_test_samples = len(split.test_classed_labels)\n",
      "                if not wavenum in pearsons:\n",
      "                    pearsons[wavenum] = []\n",
      "                pearsons[wavenum].append(pearsonr(predictions, actual))\n",
      "                for test_idx in range(n_test_samples):\n",
      "                    sample_id = split.test_id[test_idx]\n",
      "                    if not sample_id in aging_scores:\n",
      "                        aging_scores[sample_id] = {}\n",
      "                    wave = 'wave_'+str(wavenum)\n",
      "                    if not wave in aging_scores[sample_id]:\n",
      "                        aging_scores[sample_id][wave] = {'A':actual[test_idx], 'P':[]}\n",
      "                    aging_scores[sample_id][wave]['P'].append(predictions[test_idx])\n",
      "            except np.linalg.LinAlgError as err:\n",
      "                print ('split',split.get_fname (wavenum),err)\n",
      "                sys.stdout.flush()\n",
      "\n",
      "    p.animate (step)\n",
      "    for wavenum in pearsons.keys():\n",
      "        print('R^2 Average, wave',wavenum,':', np.sum(np.square(pearsons[wavenum]))/len(pearsons[wavenum]))\n",
      "        sys.stdout.flush()\n",
      "    return (aging_scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Filter_Features (num_splits):\n",
      "    feat_dict = {}\n",
      "    # (1,4) for all 3 waves\n",
      "    for wavenum in range(1,4):\n",
      "        if wavenum == 1:\n",
      "            Split.filename_base = \"sard_w1_split_120tr_13te\"\n",
      "            raw_data = read_data_updated ('newWave1.csv', 'features')\n",
      "            ages = raw_data.Age.values\n",
      "            data_matrix, col_names, age_col_idx, id_col_idx = clean_convert(raw_data)\n",
      "            col_names = np.delete(col_names, [age_col_idx, id_col_idx])\n",
      "        if wavenum == 2:\n",
      "            Split.filename_base = \"sard_w2_split_96tr_10te\"\n",
      "            raw_data = read_w2 ('newWave2.csv')\n",
      "            ages = raw_data.Age.values\n",
      "            data_matrix, col_names, age_col_idx, id_col_idx = clean_convert(raw_data)\n",
      "            col_names = np.delete(col_names, [age_col_idx, id_col_idx])\n",
      "            print (len(col_names))\n",
      "        if wavenum == 3:\n",
      "            Split.filename_base = \"sard_w3_split_82tr_8te\"\n",
      "            raw_data = read_data_updated ('newWave3.csv', 'samples')\n",
      "            ages = raw_data.Age.values\n",
      "            data_matrix, col_names, age_col_idx, id_col_idx = clean_convert(raw_data)\n",
      "            col_names = np.delete(col_names, [age_col_idx, id_col_idx])\n",
      "        split = Split()\n",
      "        fisher_weights = []\n",
      "        pearson_weights = []\n",
      "        mrmr_weights = []\n",
      "        for i in range(num_splits):\n",
      "            if i == 1:\n",
      "                print(split.train_set.shape[1])\n",
      "            split.read(split.get_fname (i))\n",
      "            pearson_weights.append(Pearson(split))\n",
      "            fisher_weights.append(Fisher(split))\n",
      "            mrmr_weights.append(read_mrmr(wavenum, i))\n",
      "        fisher_weights = np.mean(np.vstack(fisher_weights), axis=0)\n",
      "        pearson_weights = np.mean(np.vstack(pearson_weights), axis=0)\n",
      "        mrmr_weights = np.mean(np.vstack(mrmr_weights), axis=0)\n",
      "        for feat_idx in range(len(col_names)):\n",
      "            feat_name = col_names[feat_idx]\n",
      "            if not feat_name in feat_dict:\n",
      "                feat_dict[feat_name] = {}\n",
      "            wave_name  = 'wave'+str(wavenum)\n",
      "            if not wave_name in feat_dict[feat_name]:\n",
      "                feat_dict[feat_name][wave_name] = {'P':[], 'F':[], 'M':[]}\n",
      "                feat_dict[feat_name][wave_name]['P'].append(pearson_weights[feat_idx])\n",
      "                feat_dict[feat_name][wave_name]['F'].append(fisher_weights[feat_idx])\n",
      "                feat_dict[feat_name][wave_name]['M'].append(mrmr_weights[feat_idx])\n",
      "    return (feat_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Normalize_Feature_Scores (feature_weights):\n",
      "    norm_weights = np.divide(feature_weights,(np.ones(len(feature_weights))*max(feature_weights)))\n",
      "    return (norm_weights)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Find_Common_Features ():\n",
      "    common_col_names_dict = {}\n",
      "    common_col_names_list = []\n",
      "    \n",
      "    w1_data = read_data_updated ('newWave1.csv', 'features')\n",
      "    w1_ages = w1_data.Age.values\n",
      "    data_matrix, w1_col_names, w1_age_col_idx, w1_id_col_idx = clean_convert(w1_data)\n",
      "    w2_data = read_w2 ('newWave2.csv')\n",
      "    w2_ages = w2_data.Age.values\n",
      "    data_matrix, w2_col_names, w2_age_col_idx, w2_id_col_idx = clean_convert(w2_data)\n",
      "    w3_data = read_data_updated ('newWave3.csv', 'samples')\n",
      "    w3_ages = w3_data.Age.values\n",
      "    data_matrix, w3_col_names, w3_age_col_idx, w3_id_col_idx = clean_convert(w3_data)\n",
      "    \n",
      "    for feat_name in w1_col_names:\n",
      "        if not feat_name in common_col_names_dict:\n",
      "            common_col_names_dict[feat_name] = []\n",
      "        common_col_names_dict[feat_name].append('1')\n",
      "    \n",
      "    for feat_name in w2_col_names:\n",
      "        if not feat_name in common_col_names_dict:\n",
      "            common_col_names_dict[feat_name] = []\n",
      "        common_col_names_dict[feat_name].append('2')\n",
      "        \n",
      "    for feat_name in w3_col_names:\n",
      "        if not feat_name in common_col_names_dict:\n",
      "            common_col_names_dict[feat_name] = []\n",
      "        common_col_names_dict[feat_name].append('3')\n",
      "        \n",
      "    for feat_name in common_col_names_dict:\n",
      "        if len(common_col_names_dict[feat_name]) == 3:\n",
      "            common_col_names_list.append(feat_name)\n",
      "        \n",
      "    return (common_col_names_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Harmonize_Features (common_col_names, wavenum, split):\n",
      "    if wavenum == 1:\n",
      "        data = read_data_updated ('newWave1.csv', 'features')\n",
      "        ages = data.Age.values\n",
      "        data_matrix, col_names, age_col_idx, id_col_idx = clean_convert(data)\n",
      "        new_col_names = np.delete(col_names, [age_col_idx, id_col_idx])\n",
      "    elif wavenum == 2:\n",
      "        data = read_w2 ('newWave2.csv')\n",
      "        ages = data.Age.values\n",
      "        data_matrix, col_names, age_col_idx, id_col_idx = clean_convert(data)\n",
      "        new_col_names = np.delete(col_names, [age_col_idx, id_col_idx])\n",
      "    elif wavenum == 3:\n",
      "        data = read_data_updated ('newWave3.csv', 'samples')\n",
      "        ages = data.Age.values\n",
      "        data_matrix, col_names, age_col_idx, id_col_idx = clean_convert(data)\n",
      "        new_col_names = np.delete(col_names, [age_col_idx, id_col_idx])\n",
      "    else:\n",
      "        return ('Wave Number is out of bounds: Use 1, 2, or 3')\n",
      "    \n",
      "    delete_idxs = []\n",
      "    \n",
      "    for idx, feat_name in enumerate(new_col_names):\n",
      "        if not feat_name in common_col_names:\n",
      "            delete_idxs.append(idx)\n",
      "    \n",
      "    harmonized_train_set = np.delete(split.train_set, delete_idxs, axis=1)\n",
      "    harmonized_test_set = np.delete(split.test_set, delete_idxs, axis=1)\n",
      "    \n",
      "    return (harmonized_train_set, harmonized_test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###ProgressBar\n",
      "```python\n",
      "p = Progressbar(120)\n",
      "for i in range(1, 120+1):\n",
      "    p.animate(i)\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys, time\n",
      "try:\n",
      "    from IPython.display import clear_output\n",
      "    have_ipython = True\n",
      "except ImportError:\n",
      "    have_ipython = False\n",
      "\n",
      "class ProgressBar(object):\n",
      "    def __init__(self, iterations):\n",
      "        self.iterations = iterations\n",
      "        self.prog_bar = '[]'\n",
      "        self.fill_char = '*'\n",
      "        self.width = 20\n",
      "        self.__update_amount(0)\n",
      "        if have_ipython:\n",
      "            self.animate = self.animate_ipython\n",
      "        else:\n",
      "            self.animate = self.animate_noipython\n",
      "\n",
      "    def animate_ipython(self, iter):\n",
      "        print ('\\r', self, end=\"\")\n",
      "        sys.stdout.flush()\n",
      "        self.update_iteration(iter + 1)\n",
      "        if (iter + 1 > self.iterations):\n",
      "            print ()\n",
      "\n",
      "    def update_iteration(self, elapsed_iter):\n",
      "        self.__update_amount((elapsed_iter / float(self.iterations)) * 100.0)\n",
      "        self.prog_bar += '  %d of %s complete ' % (elapsed_iter, self.iterations)\n",
      "\n",
      "    def __update_amount(self, new_amount):\n",
      "        percent_done = int(round((new_amount / 100.0) * 100.0))\n",
      "        all_full = self.width - 2\n",
      "        num_hashes = int(round((percent_done / 100.0) * all_full))\n",
      "        self.prog_bar = '[' + self.fill_char * num_hashes + ' ' * (all_full - num_hashes) + ']'\n",
      "        pct_place = (len(self.prog_bar) // 2) - len(str(percent_done))\n",
      "        pct_string = '%d%%' % percent_done\n",
      "        self.prog_bar = self.prog_bar[0:pct_place] + \\\n",
      "            (pct_string + self.prog_bar[pct_place + len(pct_string):])\n",
      "\n",
      "    def __str__(self):\n",
      "        return str(self.prog_bar)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}